# -*- coding: utf-8 -*-
"""project769.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ncBQHf8l9tyw4TEbShZKC9uQq3wstGeM

# Team Members

Abhijeet Manoj Pal (200100107)

Kanak Yadav (20d070044)

Gnanendar Reddy (200070053)

#Imports
"""

# importing libraries, modules, and some useful functions
import cv2
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
import tensorflow.keras.layers as L
from tensorflow.keras.utils import img_to_array

from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.applications import VGG16
from tensorflow.keras.applications import ResNet50

from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score

import torch
from torch.nn import Sequential # making the feature extractor
from torchvision.models import resnet18, ResNet18_Weights
from torchvision import datasets, models, transforms

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

"""# Functions"""

# function to load a single image from the dataset
def load_image(id):
    img = cv2.imread(str(id) + '.jpg')
    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# function to load multiple images to give input to the models
def load_images(ids, img_size):
    images = np.ndarray(shape=(len(ids), img_size, img_size, 3), dtype=np.float32)
    for i in range(len(ids)):
        temp = cv2.imread(str(ids[i]) + '.jpg')
        temp = cv2.resize(temp, (img_size, img_size), interpolation=cv2.INTER_AREA)
        images[i] = img_to_array(temp)
    return images/255.0

# function to plot the accuracy for every epoch while training the deep learning models
def plot_train(history):
    plt.plot(range(1, EPOCHS + 1), history.history['categorical_accuracy'])
    plt.plot(range(1, EPOCHS + 1), history.history['val_categorical_accuracy'])
    plt.xticks(range(0, EPOCHS + 1, EPOCHS//5 + 1))
    plt.legend(['Training', 'Validations'])
    plt.grid(linestyle='--')
    plt.title('Accuracy vs Epochs')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.show()
    return

# function to show the results of a trained model on the test data
def show_results(model):
    preds = model.predict(x_test)
    try: pred = np.argmax(preds, axis=1)
    except: pred = preds
    try: test = np.argmax(y_test, axis=1)
    except: test = y_test
    print(f"Accuracy: {accuracy_score(test, pred)}, \nF1 score: {f1_score(test, pred, average='weighted')}")
    ConfusionMatrixDisplay(confusion_matrix(test, pred), display_labels=cols).plot()
    plt.show()
    return

# function to compute the features of an image using resnet18 as a feature extractor
def features(id, img_size, model):
    img = cv2.resize(cv2.imread(str(id) + '.jpg'), (img_size, img_size), interpolation=cv2.INTER_AREA)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype('float32') / 255.0
    img -= MEAN
    img /= STD
    img = np.transpose(img, (2, 0, 1))
    img = np.expand_dims(img,  0)
    return model(torch.from_numpy(img)).detach().numpy()[0, :, 0, 0]

"""# Dataset overview"""

# Commented out IPython magic to ensure Python compatibility.
# accessing the images from google drive
from google.colab import drive
drive.mount('/content/gdrive')
# %cd gdrive/MyDrive/project769

# loading the file with class labels for each image
df = pd.read_csv('train.csv')
cols = df.columns[1:]
df.info()

# we see that one-hot bit encoding has been done for the multiple classes
df.head()

# Commented out IPython magic to ensure Python compatibility.
# displaying a few sample images belonging to each of the classes
# %cd images
for j in range(4):
    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(24, 6))
    for i in range(4):
        ax[i].imshow(load_image(df.image_id[df[cols[i]] == 1].iloc[j]))
        if j == 0: ax[i].title.set_text(cols[i])

"""# Data Splitting"""

# image dimensions for input to the models
img_size = 224

# spliiting the data in training validation and testing
x_train, x_test, y_train, y_test = map(lambda x: np.array(x.values), train_test_split(df.image_id, df[cols], test_size = 0.15))
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.3)

# loading all the images into memory
x_train, x_val, x_test = map(lambda x: load_images(x, img_size), (x_train, x_val, x_test))

"""# Densenet121"""

# parameters for training
EPOCHS = 20
BATCH_SIZE = 32
STEPS_PER_EPOCH = y_train.shape[0] // BATCH_SIZE

# defining the Densenet121 model with custom output layer (for four classes)
model = tf.keras.Sequential(
    [
    DenseNet121(input_shape=(img_size, img_size, 3), weights='imagenet', include_top=False),
    L.GlobalAveragePooling2D(),
    L.Dense(y_train.shape[1], activation='softmax')
    ])
    
model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['categorical_accuracy'])
model.summary()

# training the model
history = model.fit(x = x_train, y = y_train, epochs = EPOCHS,
                    steps_per_epoch = y_train.shape[0] // BATCH_SIZE,
                    validation_data = (x_val, y_val))

# plotting the training and validation accuracy for every epoch
plot_train(history)

# showing the results on the test data: accuracy, f1-score and the confusion matrix
show_results(model)

"""# VGG16"""

# parameters for training
EPOCHS = 20
BATCH_SIZE = 32
STEPS_PER_EPOCH = y_train.shape[0] // BATCH_SIZE

# defining the VGG16 model with custom output layer (for four classes)
model = tf.keras.Sequential(
    [
    VGG16(input_shape=(img_size, img_size, 3), weights='imagenet', include_top=False),
    L.GlobalAveragePooling2D(),
    L.Dense(y_train.shape[1], activation='softmax')
    ])
    
model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['categorical_accuracy'])
model.summary()

# training the model
history = model.fit(x = x_train, y = y_train, epochs = EPOCHS,
                    steps_per_epoch = y_train.shape[0] // BATCH_SIZE,
                    validation_data = (x_val, y_val))

# plotting the training and validation accuracy for every epoch
plot_train(history)

# showing the results on the test data: accuracy, f1-score and the confusion matrix
show_results(model)

"""# Resnet50"""

# parameters for training
EPOCHS = 20
BATCH_SIZE = 32
STEPS_PER_EPOCH = y_train.shape[0] // BATCH_SIZE

# defining the ResNet50 model with custom output layer (for four classes)
model = tf.keras.Sequential(
    [
    ResNet50(input_shape=(img_size, img_size, 3), weights='imagenet', include_top=False),
    L.GlobalAveragePooling2D(),
    L.Dense(y_train.shape[1], activation='softmax')
    ])
    
model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics=['categorical_accuracy'])
model.summary()

# training the model
history = model.fit(x = x_train, y = y_train, epochs = EPOCHS,
                    steps_per_epoch = y_train.shape[0] // BATCH_SIZE,
                    validation_data = (x_val, y_val))

# plotting the training and validation accuracy for every epoch
plot_train(history)

# showing the results on the test data: accuracy, f1-score and the confusion matrix
show_results(model)

"""# Transferred Learning"""

# mean ans standard deviation of the Imagenet dataset for normalizing or images
MEAN = [0.485, 0.456, 0.406]
STD = [0.229, 0.224, 0.225]

# importing the pre-trained resnet18 model to be used as a feature ectractor, hence removed the last layer
resnet_ = Sequential(*list(resnet18(weights=ResNet18_Weights).children())[:-1])
# splitting the dataset into training and testing
x_train, x_test, y_train, y_test = train_test_split(np.array(df.image_id.values), np.array(df[cols].values), test_size=0.30)
# obtaining the features for all the images to be used as the input to the simple machine learning models
x_train = list(map(lambda x: features(x, img_size, resnet_), x_train))
x_test = list(map(lambda x: features(x, img_size, resnet_), x_test))
# converting the label vectors into labels for the models
y_train, y_test = map(lambda x: np.argmax(x, axis=1), (y_train, y_test))

# training a Logistic Regressor on the dataset
model = LogisticRegression(class_weight='balanced')
model.fit(x_train, y_train)

# showing the results on the test data: accuracy, f1-score and the confusion matrix
show_results(model)

# training a Support Vector Classifier on the dataset
model = SVC()
model.fit(x_train, y_train)

# showing the results on the test data: accuracy, f1-score and the confusion matrixshow_results(model)

# training a Simple Neural Network with a single hidden layer on the dataset
model = MLPClassifier()
model.fit(x_train, y_train)

# showing the results on the test data: accuracy, f1-score and the confusion matrix
show_results(model)

"""## References

1. R. Dhivya, R Vennila, G Rohini, S Mithila, K Kavitha ``Foilar Disease Classification in Apple Trees,'' 2021 International Conference on Advancements in Electrical, Electronics, Communication, Computing and Automation
2. Kaggle Dataset used for the project Link: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models
3. ResNet50, VGG16, DenseNet121 tutorials used Link : https://keras.io/api/applications/
4. Inspiration: https://www.kaggle.com/code/tarunpaparaju/plant-pathology-2020-eda-models#Modeling-
"""